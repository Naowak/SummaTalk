{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "\n",
    "def make_completion_request(prompt):\n",
    "    url = 'http://localhost:8000/v1/completions'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def make_several_completion_requests(prompts):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(make_completion_request, prompts)\n",
    "    results = list(results)\n",
    "    return results\n",
    "\n",
    "prompts = [\"Résume moi la vie de Babar\", \"Qu'est-ce que le Machine Learning?\", \"Explique le concept de récursivité\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = make_several_completion_requests(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'cmpl-3050137d84f546a2ae03ab6905899e06',\n",
       "  'object': 'text_completion',\n",
       "  'created': 429392,\n",
       "  'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "  'choices': [{'index': 0,\n",
       "    'text': \", un éléphant élégant et raffiné qui règne sur une grande forêt et qui a été élevé par une belle princesse humaine.\\n\\nBabar est un éléphant élégant et raffiné qui a été élevé par une belle princesse humaine dans sa jeunesse. Il règne maintenant sur une grande forêt composée d'éléphants, et il est connu pour sa gracieuse apparence et sa gentillesse.\\n\\nBabar est né dans la forêt des éléphants sauvages, mais il a été trouvé par la princesse humaine dans ses premiers jours de vie. Elle l'a adopté et l'a élevé comme un de ses propres enfants, lui enseignant les manières et les coutumes humaines. Babar a appris à parler, à lire et à écrire, et il a adopté les vêtements et les habits de la cour royale humaine.\\n\\nQuand Babar a grandi, il a décidé de retourner dans sa forêt natale pour devenir roi des éléphants. Les éléphants étaient initialement réticents à accepter un roi éléphant élevé par des humains, mais Babar a utilisé ses nouvelles compétences et sa gracieuse apparence pour les persuader de l'accepter. Il a également apporté des bienfaits à la forêt en améliorant les infrastructures et en créant des lois et des règlements pour maintenir l'ordre et la paix.\\n\\nBabar a régné sur sa forêt avec grande succès, et il a été aimé et respecté par tous les éléphants. Il a continué à visiter la cour royale humaine de temps en temps, et il a gardé sa connection avec la culture humaine. Babar est devenu un symbole de l'harmonie entre l'humanité et la nature, et sa légende a été transmise de génération en génération.\\n\\nVoilà la vie de Babar, l'éléphant élégant et raffiné qui a régné sur une grande forêt et a été élevé par une belle princesse humaine.\",\n",
       "    'logprobs': None,\n",
       "    'finish_reason': 'stop'}],\n",
       "  'usage': {'prompt_tokens': 11,\n",
       "   'total_tokens': 553,\n",
       "   'completion_tokens': 542}},\n",
       " {'id': 'cmpl-ca8224cd84a34fb4af8a1e86e88c08f2',\n",
       "  'object': 'text_completion',\n",
       "  'created': 429392,\n",
       "  'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "  'choices': [{'index': 0,\n",
       "    'text': \"\\n\\nMachine Learning (ML) est un sous-ensemble d'intelligence artificielle (IA) qui permet aux ordinateurs de trouver des fonctions et des modèles à partir de données, en les apprenant par eux-mêmes. Dans d'autres mots, ML permet à des ordinateurs d'apprendre et de s'améliorer automatiquement grâce aux données.\\n\\nMachine learning est une forme de reconnaissance de patrons qui utilise des algorithmes pour permettre à un ordinateur d'apprendre automatiquement à identifier des structures dans des jeux de données. Il s'agit de la méthode la plus courante pour l'apprentissage automatique et la reconnaissance de formes dans les données numériques. Les méthodes de machine learning peuvent être classées en fonction de la quantité et de la nature de l'interaction requise avec les données d'apprentissage.\\n\\nQuels sont les types de Machine Learning?\\n\\nIl existe trois types principaux de machine learning:\\n\\n1. Apprentissage supervisé: L'ordinateur apprend à classer, prédire ou modeller des données en fonction d'étiquettes de données d'entrée connues. Il s'agit d'un apprentissage sous-ensemble et est utilisé pour résoudre des problèmes tels que la classification et la régression.\\n2. Apprentissage non supervisé: L'ordinateur recherche des structures et des modèles dans des données sans étiquettes. Il s'agit d'un apprentissage sous-ensemble et est utilisé pour résoudre des problèmes tels que la clusterisation et la décomposition de la matrice.\\n3. Apprentissage réinforcement: L'ordinateur apprend à prendre des décisions en fonction de récompenses et de pénalités dans un environnement dynamique. Il s'agit d'un apprentissage par renforcement et est utilisé pour résoudre des problèmes tels que le contrôle de robot et la planification optimale.\\n\\nQuels sont les applications de Machine Learning?\\n\\nMachine learning a des applications dans de nombreuses industries et domaines, y compris:\\n\\n1. Systèmes de recommandation: Les systèmes de recommandation de Netflix, Amazon et YouTube utilisent des algorithmes de machine learning pour recommander des films, des produits et des vidéos à leurs clients.\\n2. Reconnaissance de parole: Les assistants vocaux tels que Siri, Google Assistant et Alexa utilisent des algorithmes de machine learning pour reconnaître et comprendre les mots et les phrases humaines.\\n3. Reconnaissance faciale: Les systèmes de reconnaissance faciale tels que FaceID et Google Photos utilisent des algorithmes de machine learning pour reconnaître et identifier des visages dans des images.\\n4. Systèmes de traitement médical: Les systèmes de traitement médical tels que IBM Watson et Google Health utilisent des algorithmes de machine learning pour diagnostiquer et traiter des maladies.\\n5. Jeux et stratégie: Les algorithmes de machine learning sont utilisés pour jouer aux jeux d'échecs, aux jeux de go et aux jeux de poker pour battre les humains.\\n6. Analyse financière: Les algorithmes de machine learning sont utilisés pour analyser les données financières et prévoir les tendances boursières.\\n7. Traitement d'images et de vidéos: Les algorithmes de machine learning sont utilisés pour analyser et classer des images et des vidéos, par exemple pour détecter des anomalies ou des événements spécifiques.\\n\\nQuels sont les avantages de l'apprentissage automatique?\\n\\nLes avantages de l'apprentissage automatique sont:\\n\\n1. Amélioration continue: Les modèles de machine learning améliorent leur performance en apprenant des données supplémentaires et en s'adaptant aux nouveaux données.\\n2. Prédiction précise: Les modèles de machine learning peuvent apprendre à prédire des résultats précis à partir de données, ce qui peut aider à prendre des décisions informées.\\n3. Réduction des coûts: Les modèles de machine learning peuvent automatiser des processus complexes, réduisant ainsi les coûts et les temps d'exécution.\\n4. Scalabilité: Les modèles de machine learning peuvent être utilisés pour traiter des données volumineuses et complexes, ce qui est difficile à faire à la main.\\n5. Flexibilité: Les modèles de machine learning peuvent être utilisés pour résoudre une variété de problèmes, de la classification simple à la prédiction complexe.\\n6. Adaptabilité: Les modèles de machine learning peuvent être adaptés à de nouveaux environnements et à de nouveaux types de données, ce qui les rend très adaptables.\\n\\nQuels sont les défis de l'apprentissage automatique?\\n\\nLes défis de l'apprentissage automatique sont:\\n\\n1. Génération de données d'apprentissage: Les modèles de machine learning nécessitent des données d'apprentissage abondantes et diverses pour apprendre efficacement.\\n2. Qualité des données: Les modèles de machine learning sont sensibles aux données de mauvaise qualité, telles que les données manquantes, les données imparfaites ou les données biaisées.\\n3. Interprétabilité: Les modèles de machine learning peuvent être difficiles à interpréter et à expliquer, ce qui peut être un obstacle à leur utilisation dans certaines applications.\\n4. Sécurité et privé: Les modèles de machine learning peuvent être utilisés pour collecter et traiter des données personnelles, ce qui peut entraîner des problèmes de confidentialité et de sécurité.\\n5. Biais et discrimination: Les modèles de machine learning peuvent retransmettre des biais et des discriminations présents dans les données d'apprentissage, ce qui peut entraîner des résultats injustes.\\n6. Éthique: Les modèles de machine learning peuvent être utilisés dans des applications controversées, telles que la surveillance des citoyens ou la manipulation des élections, ce qui pose des questions éthiques.\\n\\nQuels sont les outils et les frameworks les plus populaires pour le Machine Learning?\\n\\nLes plus populaires outils et frameworks pour le machine learning sont:\\n\\n1. TensorFlow: TensorFlow est un framework open-source développé par Google pour le machine learning et la deep learning. Il permet la création et l'entraînement de modèles complexes, ainsi que la distribution de calcul sur plusieurs ordinateurs.\\n2. PyTorch: PyTorch est un framework open-source développé par Facebook pour le machine learning et la deep learning. Il est particulièrement populaire pour le deep learning et la recherche en machine learning.\\n3. Scikit-learn: Scikit-learn est un framework open-source pour le machine learning basé sur Python. Il offre une gamme large de classes de modèles et de fonctions utilitaires pour le traitement de données et la préparation des données.\\n4. Keras: Keras est un framework open-source pour le deep learning développé par Google. Il est simple à utiliser et permet de réaliser des expériences de deep learning rapides et efficaces.\\n5. Theano: Theano est un framework open-source pour le machine learning et le deep learning développé par la Université de Montréal et par Institut MILA. Il est particulièrement populaire pour son support avancé pour le GPU et sa vitesse de calcul.\\n\\nQuels sont les ressources pour apprendre le Machine Learning?\\n\\nIl existe de nombreuses ressources pour apprendre le machine learning, y compris:\\n\\n1. Coursera: Coursera offre des cours en ligne gratuits et à prix réduits sur le machine learning et d'autres domaines.\\n2. edX: edX offre des cours en ligne gratuits et à prix réduits sur le machine learning et d'autres domaines, en partenariat avec des universités et des institutions prestigieuses.\\n3. Stanford University: Stanford University offre des cours en ligne gratuits sur le machine learning et d'autres domaines.\\n4. Google: Google offre des cours et des ressources en ligne sur le machine learning et d'autres technologies.\\n5. Microsoft: Microsoft offre des cours et des ressources en ligne sur le machine learning et d'autres technologies.\\n6. O'Reilly: O'Reilly offre des livres, des cours et des ressources sur le machine learning et d'autres technologies.\\n7. Towards Data Science: Towards Data Science est une publication en ligne qui publie des articles et des tutoriels sur le data science, y compris le machine learning.\\n8. Kaggle: Kaggle est une plateforme de compétition en data science et en machine learning, où les participants peuvent participer à des compétitions et gagner des récompenses.\\n9. GitHub: GitHub est une plateforme de partage de code où vous pouvez trouver des projets, des bibliothèques et des ressources pour le machine learning.\\n\\nQuels sont les carrières possibles dans le Machine Learning?\\n\\nLes carrières possibles dans le machine learning sont:\\n\\n1. Data Scientist: Un data scientist est responsable de collecter, de traiter et d'analyser des données pour insuffler de l'intelligence aux entreprises et aux organisations.\\n2. Machine Learning Engineer: Un machine learning engineer est responsable de la conception et de l'implémentation de modèles de machine learning et de deep learning, ainsi que de la création et de la maintenance de plates-formes de machine learning.\\n3. Research Scientist: Un research scientist est responsable de la recherche et du développement de nouveaux algorithmes et de modèles de machine learning et de deep learning.\\n4. Data Engineer: Un data engineer est responsable de la conception et de l'implémentation de systèmes de stockage et de traitement de données pour prendre en charge les besoins de données des data scientists et des machine learning engineers.\\n5. Deep Learning Engineer: Un deep learning engineer est responsable de la conception et de l'implémentation de modèles de deep learning complexes et de la création et de la maintenance de plates-formes de deep learning.\\n6. Product Manager: Un product manager est responsable de la gestion de produit pour des solutions de machine learning et de deep learning.\\n7. Solutions Architect: Un solutions architect est responsable de la conception et de l'implémentation d'architectures de solution pour des solutions de machine learning et de deep learning.\\n8. DevOps Engineer: Un devops engineer est responsable de la gestion des opérations et de la sécurité pour des plates-formes de machine learning et de deep learning.\\n9. Researcher: Un researcher est responsable de la recherche et du développement de nouveaux algorithmes et de modèles de machine learning et de deep learning pour des applications industrielles et academiques.\\n\\nQuels sont les compétences et les connaissances nécessaires pour travailler dans le Machine Learning?\\n\\nLes compétences et les connaissances nécessaires pour travailler dans le machine learning sont:\\n\\n1. Programmation: Une solide compréhension de programmation, en particulier en Python et en R, est nécessaire pour travailler dans le machine learning.\\n2. Mathématiques: Une solide compréhension des mathématiques sous-jacentes au machine learning, telles que les statistiques, la théorie des probabilités et la linéaire algébrique, est nécessaire.\\n3. Data Manipulation: Une solide compréhension des techniques de traitement de données, telles que le nettoyage des données, la préparation des données et la transformation des données, est nécessaire.\\n4. Algorithms: Une solide compréhension des algorithmes de machine learning, tels que le regression linéaire, le regression logistique, le support vector machines et la neural networks, est nécessaire.\\n5. Deep Learning: Une connaissance des concepts et des techniques fondamentaux du deep learning, tels que les réseaux neuronaux profonds, les convolutional neural networks et les réseaux recurrentes, est de plus en plus importante.\\n6. Data Visualization: Une connaissance des techniques de visualisation de données, telles que le graphing et le plotting, est nécessaire pour interpréter et présenter les résultats de la recherche en machine learning.\\n7. Cloud Computing: Une connaissance des plateformes de cloud computing, telles que Amazon Web Services, Microsoft Azure et Google Cloud Platform, est nécessaire pour exécuter des calculs intensifs et des modèles de machine learning à grande échelle.\\n8. Data Science Libraries: Une connaissance des bibliothèques de data science, telles que Scikit-learn, TensorFlow et PyTorch, est nécessaire pour travailler avec les données et les modèles de machine learning.\\n9. Communication: Une bonne communication écrite et orale est nécessaire pour expliquer les résultats de la recherche en machine learning aux collègues, aux clients et aux partenaires.\\n10. Problématique: Une bonne compréhension des domaines d'application du machine learning, telles que la finance, la santé, l'éducation et l'industrie, est nécessaire pour appliquer les techniques de machine learning appropriées aux problèmes réels.\",\n",
       "    'logprobs': None,\n",
       "    'finish_reason': 'stop'}],\n",
       "  'usage': {'prompt_tokens': 11,\n",
       "   'total_tokens': 3285,\n",
       "   'completion_tokens': 3274}},\n",
       " {'id': 'cmpl-b13cf76f604a491abce257fdb266f0b1',\n",
       "  'object': 'text_completion',\n",
       "  'created': 429392,\n",
       "  'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "  'choices': [{'index': 0,\n",
       "    'text': ' et donnez quelques exemples en programmation.\\n\\nThe concept of recursion is a powerful programming technique where a function calls itself repetitively until a terminating condition is met. This technique can be used to simplify complex problems by breaking them down into smaller sub-problems of the same form.\\n\\nIn mathematical terms, a recursive function is defined in terms of itself. It has a base case, which is the terminating condition, and one or more recursive cases, which call the function with smaller inputs until the base case is reached.\\n\\nLet\\'s look at some examples of recursion in programming:\\n\\n1. Factorial function:\\nThe factorial of a number n is the product of all positive integers less than or equal to n. This can be calculated recursively as follows:\\n\\n```java\\nint factorial(int n) {\\n    if (n == 0) {\\n        return 1; // base case\\n    } else {\\n        return n * factorial(n-1); // recursive case\\n    }\\n}\\n```\\n\\n2. Fibonacci sequence:\\nThe Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones. It can be calculated recursively as follows:\\n\\n```java\\nint fibonacci(int n) {\\n    if (n <= 1) {\\n        return n; // base case\\n    } else {\\n        return fibonacci(n-1) + fibonacci(n-2); // recursive case\\n    }\\n}\\n```\\n\\n3. Tower of Hanoi:\\nThe Tower of Hanoi is a classic mathematical puzzle. The problem can be solved recursively as follows:\\n\\n```java\\nvoid towerOfHanoi(int n, char fromRod, char toRod, char auxRod) {\\n    if (n > 0) {\\n        towerOfHanoi(n-1, fromRod, auxRod, toRod);\\n        System.out.println(\"Move disk \" + n + \" from rod \" + fromRod + \" to rod \" + toRod);\\n        towerOfHanoi(n-1, auxRod, toRod, fromRod);\\n    }\\n}\\n```\\n\\nIn this example, the `towerOfHanoi` function recursively moves `n` disks from one rod to another by using a third rod as auxiliary. The base case is when there is no disk to move, i.e., `n == 0`.',\n",
       "    'logprobs': None,\n",
       "    'finish_reason': 'stop'}],\n",
       "  'usage': {'prompt_tokens': 10,\n",
       "   'total_tokens': 583,\n",
       "   'completion_tokens': 573}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-43d379804c38441482401ab237c93466', 'object': 'text_completion', 'created': 429254, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'choices': [{'index': 0, 'text': \" l'éléphant\\n\\nBabar est un éléphant blanc né dans la forêt vierge. Il est élevé par sa mère, la reine des éléphants, qui l'apprête à devenir roi. Quand celle-ci est tuée par un chasseur, Babar est très triste et décide de quitter la forêt. Il rencontre une princesse rose et s'enfume. Il décide de s'installer à Paris, où il rencontre des humains. Il s'habille avec des vêtements humains et se voit accorder la main de la princesse. Ils se marient et Babar devient le roi de Paris. Ils ont deux enfants, Alexandre et Marie. Babar continue à aller dans sa forêt natale et à s'occuper de ses sujets éléphants. Il a également une tante, Mme Saxo, qui vit dans une tour et qui aide à élever ses enfants.\\n\\nBabar's Life, Summed Up\\n\\nBabar is a white elephant born in the forest. He is raised by his elephant queen mother, who prepares him to become king. When she is hunted and killed by a hunter, Babar becomes sad and leaves the forest. He encounters a pink princess and falls in love. He decides to move to Paris, where he meets humans. He adopts human clothing and marries the princess. They have two children, Alexander and Marie. Babar continues to visit his native forest and care for his elephant subjects. He also has an aunt, Madame Saxo, who lives in a tower and helps raise his children.\", 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 11, 'total_tokens': 386, 'completion_tokens': 375}}\n",
      "{'id': 'cmpl-45a533f5d1684c5ba6f69a326578cbd5', 'object': 'text_completion', 'created': 429254, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'choices': [{'index': 0, 'text': \"\\n\\nMachine Learning est une branche de l'intelligence artificielle (IA) qui utilise des algorithmes et des modèles statistiques pour permettre à un ordinateur de découvrir des informations cachées dans des données et de les utilisées pour faire des prédictions ou des décisions automatiquement, sans être explicitement programmé à le faire.\\n\\nQuel est le but de Machine Learning?\\n\\nLe but de Machine Learning est de permettre à un ordinateur de découvrir des informations cachées dans des données et de les utiliser pour faire des prédictions ou des décisions automatiquement, sans être explicitement programmé à le faire. Cela permet de résoudre des problèmes complexes tels que la reconnaissance de formes, la classification automatique de données, la prévision de tendances et la détection de fraudes.\\n\\nQuels sont les différents types de Machine Learning?\\n\\nIl existe trois grands types de Machine Learning:\\n\\n1. Apprentissage supervisé: L'ordinateur est fourni avec des données d'entrainement et des réponses correctes, et il apprend à déterminer les fonctionnalités importantes et à mapping les entrées à des sorties correctes.\\n2. Apprentissage non supervisé: L'ordinateur est fourni uniquement avec des données d'entrainement, sans aucune information sur les sorties attendues. Il tente de découvrir les structures et les relations dans les données.\\n3. Apprentissage réinforcé: L'ordinateur apprend à agir dans un environnement en réagissant à des récompenses ou des pénalités pour des actions qu'il prend.\\n\\nQuels sont les avantages de Machine Learning?\\n\\nLes principaux avantages de Machine Learning sont:\\n\\n1. L'automatisation de processus complexes: Machine Learning permet de résoudre des problèmes complexes qui seraient difficiles à résoudre à la main.\\n2. La prédictive et la décision automatique: Machine Learning permet de faire des prédictions et de prendre des décisions automatiquement à partir de données.\\n3. L'adaptabilité: Machine Learning peut apprendre et s'améliorer continuellement en analysant de nouveaux données.\\n4. L'économie de temps et d'argent: Machine Learning peut automatiser des tâches qui seraient coûteuses ou temps morts à faire à la main.\\n\\nQuels sont les défis de Machine Learning?\\n\\nLes principaux défis de Machine Learning sont:\\n\\n1. L'obtention de données de haute qualité: Les algorithmes de Machine Learning sont sensibles aux données d'entrainement de mauvaise qualité. Il est donc important de collecter des données pertinentes, précises et fiables.\\n2. Le biais et la discrimination: Les algorithmes de Machine Learning peuvent réintégrer les biais et la discrimination présents dans les données d'entrainement. Il est donc important de prendre en compte ces questions lors du développement de modèlesMachine Learning.\\n3. L'explainabilité: Les algorithmes de Machine Learning peuvent être complexes et difficiles à comprendre. Il est donc important de fournir des explications claires et concises sur comment les modèles fonctionnent et comment ils arrivent à leurs décisions.\\n4. La scalabilité: Les algorithmes de Machine Learning peuvent être complexes et coûteux à exécuter sur de grandes données. Il est donc important de développer des solutions scalables pour traiter des données volumineuses.\\n\\nQuels sont les outils populaires pour Machine Learning?\\n\\nLes outils populaires pour Machine Learning sont:\\n\\n1. TensorFlow: TensorFlow est un framework open source pour Machine Learning et d'apprentissage profond développé par Google.\\n2. Scikit-learn: Scikit-learn est un framework pour Machine Learning en Python qui fournit des outils pour la préparation des données, la sélection des modèles et la validation des modèles.\\n3. KNIME: KNIME est un plateforme open source pour l'analyses des données qui fournit des outils pour le traitement des données, la préparation des données, la sélection des modèles et la validation des modèles.\\n4. Apache Spark MLlib: Apache Spark MLlib est une bibliothèque de Machine Learning intégrée à Apache Spark qui permet de traiter des données volumineuses en parallèle.\\n5. Microsoft Azure Machine Learning: Microsoft Azure Machine Learning est une plateforme cloud pour Machine Learning qui permet de développer, déployer et gérer des modèlesMachine Learning à grande échelle.\\n\\nQuels sont les applications pratiques de Machine Learning?\\n\\nLes applications pratiques de Machine Learning sont:\\n\\n1. La reconnaissance de formes: Machine Learning est utilisé pour reconnaître des formes dans des images ou des vidéos, telles que des visages, des voitures ou des objets.\\n2. La classification automatique de données: Machine Learning est utilisé pour classer automatiquement des données en fonction de leurs caractéristiques, telles que des emails, des documents ou des images.\\n3. La prévision de tendances: Machine Learning est utilisé pour prévoir des tendances en fonction de données historiques, telles que les prévisions de ventes ou les prévisions de trafic.\\n4. La détection de fraudes: Machine Learning est utilisé pour détecter des fraudes en analyseant des transactions financières ou des données de sécurité.\\n5. La traduction automatique: Machine Learning est utilisé pour traduire automatiquement des textes en plusieurs langues en utilisant des modèles de machine à apprendre.\\n6. La reconnaissance de parole: Machine Learning est utilisé pour reconnaître la parole dans des enregistrements audio en utilisant des modèles de machine à apprendre.\\n7. Le jeu et la simulation: Machine Learning est utilisé pour créer des jeux et des simulations intelligents qui peuvent apprendre et réagir à l'environnement.\\n8. L'automatisation de processus industriels: Machine Learning est utilisé pour automatiser des processus industriels complexes tels que l'inspection de produits, la qualité des produits et la maintenance prédictive.\\n9. La santé et la médecine: Machine Learning est utilisé pour analyser des données médicales pour diagnostiquer des maladies, prévoir leur progression et suggérer des traitements.\\n10. Le marketing et la publicité: Machine Learning est utilisé pour analyser des données de marketing et de publicité pour personnaliser les contenus et les offres de manière ciblée.\", 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 11, 'total_tokens': 1635, 'completion_tokens': 1624}}\n",
      "{'id': 'cmpl-8cbd868f3c8546e7a9beb76a8a0b03e1', 'object': 'text_completion', 'created': 429254, 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'choices': [{'index': 0, 'text': \" et donner un exemple pratique\\n\\nLe concept de récursivité est une technique de programmation permettant à une fonction de s'appeler elle-même. Cette technique est utile lorsque la logique d'une fonction peut être décrite en termes récursifs, c'est-à-dire en termes de cas successifs ou de sous-problèmes similaires à celui que la fonction a pour but de résoudre.\\n\\nPar exemple, pensons à la fonction de calcul de la factorielle d'un nombre entier n. La factorielle d'un nombre entier n est le produit de tous les nombres entiers allant de 1 à n. Nous pouvons définir cette fonction de manière récursive en deux étapes :\\n\\n1. Base de récursion : Pour n égal à 0 ou 1, la fonction retourne simplement 1. En effet, la factorielle de 0 ou 1 est définie comme étant 1.\\n2. Cas récursif : Pour n supérieur à 1, la fonction calcule la factorielle de n-1 en appelant la fonction elle-même avec n-1 en paramètre, et retourne le résultat multiplié par n.\\n\\nVoici l'exemple en code Python :\\n\\n```python\\ndef factorielle(n):\\n    if n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorielle(n-1)\\n```\\n\\nCette fonction fonctionne bien pour tous les nombres entiers positifs, mais si on essaye de l'appeler avec un nombre négatif ou un nombre non entier, Python rencontrera une erreur. Pour gérer ces cas d'utilisation, il est important d'ajouter des conditions d'arrêt supplémentaires à notre fonction de manière à l'empêcher de s'enrouler dans une boucle infinie.\\n\\nLes fonctions récursives peuvent également être utilisées pour résoudre d'autres types de problèmes, tels que les calculs itératifs, les algèbres linéaire, les arbres binaires, les graphes, etc. L'avantage de cette technique est qu'elle permet de simplifier la stratégie de programmation en réduisant les problèmes complexes en sous-problèmes plus simples, ce qui peut conduire à des solutions plus courtes et plus lisibles.\", 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 10, 'total_tokens': 581, 'completion_tokens': 571}}\n"
     ]
    }
   ],
   "source": [
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" Je vais vous présenter une entreprise créée par trois français, un ancien chercheur de DeepMind et deux qui étaient chez Meta.\\n  Cette entreprise, qui n'existait pas il y a à peine huit mois, a eu le temps dans cet intervalle de faire trembler toute l'industrie de l'IA\\n  en publiant des modèles alternatifs à ChatGPT qui explosent toute la concurrence, être valorisé à presque 2 milliards de dollars.\\n  Le tout sans aucune communication, ni vidéo promotionnelle déceptive.\\n  Rien.\\n  Ce que fait cette boîte me hype tellement que je vais quasiment tous les jours sur Twitter exclusivement pour vérifier qu'ils n'ont pas fait des nouvelles annonces.\\n  Et c'est véridique.\\n  C'est vrai ?\\n  Oui.\\n  Laissez-moi vous expliquer à quel point nos petits français ont explosé le game et comment vous pourriez aussi en profiter.\\n  Pour commencer, ce que je vous propose, c'est de regarder un tableau des meilleures intelligences artificielles qui sont concurrentes à ChatGPT.\\n  Vous allez voir, il y a plein de trucs très intéressants dans ce tableau.\\n  Par exemple, on dirait que ChatGPT régresse entre plusieurs versions.\\n  Sinon, on peut voir qu'il y a aussi des scores qui sont incohérents, qui ne sont pas dans le bon ordre, c'est bizarre.\\n  Et surtout, il y a ces petites lignes jaunes.\\n  Open Hermès Mistral, Machin.\\n  Mixtral Instruct.\\n  Que des noms qui évoquent le vent finalement.\\n  Elles ne payent pas de mine.\\n  On dirait même comme ça qu'elles ne sont pas si bien classées.\\n  Mais...\\n  Ce serait passé à côté de la révolution.\\n  Et je pèse mes mots qui se cachent derrière.\\n  Déjà, il faut réaliser qu'il y a pas mal de manières de mesurer la performance d'un LLM.\\n  Mais pour faire court, c'est pas simple.\\n  Vraiment pas simple.\\n  Il y a des benchmarks qui sont en gros des listes de questions qu'on peut poser à un LLM pour vérifier ses capacités.\\n  Donc là, par exemple, petite question de philosophie.\\n  Il faut remplacer avec le bon terme.\\n  Le problème, c'est que c'est déjà arrivé que\\n  des modèles cartonnent en théorie avec des scores de fous, mais en fait,\\n  ne soient pas dingues.\\n  Ça arrive assez régulièrement.\\n  Par exemple, c'est potentiellement le cas des modèles de Google.\\n  Genre Gemini.\\n  On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n\",\n",
       "  \" Ce que fait cette boîte me hype tellement que je vais quasiment tous les jours sur Twitter exclusivement pour vérifier qu'ils n'ont pas fait des nouvelles annonces.\\n  Et c'est véridique.\\n  C'est vrai ?\\n  Oui.\\n  Laissez-moi vous expliquer à quel point nos petits français ont explosé le game et comment vous pourriez aussi en profiter.\\n  Pour commencer, ce que je vous propose, c'est de regarder un tableau des meilleures intelligences artificielles qui sont concurrentes à ChatGPT.\\n  Vous allez voir, il y a plein de trucs très intéressants dans ce tableau.\\n  Par exemple, on dirait que ChatGPT régresse entre plusieurs versions.\\n  Sinon, on peut voir qu'il y a aussi des scores qui sont incohérents, qui ne sont pas dans le bon ordre, c'est bizarre.\\n  Et surtout, il y a ces petites lignes jaunes.\\n  Open Hermès Mistral, Machin.\\n  Mixtral Instruct.\\n  Que des noms qui évoquent le vent finalement.\\n  Elles ne payent pas de mine.\\n  On dirait même comme ça qu'elles ne sont pas si bien classées.\\n  Mais...\\n  Ce serait passé à côté de la révolution.\\n  Et je pèse mes mots qui se cachent derrière.\\n  Déjà, il faut réaliser qu'il y a pas mal de manières de mesurer la performance d'un LLM.\\n  Mais pour faire court, c'est pas simple.\\n  Vraiment pas simple.\\n  Il y a des benchmarks qui sont en gros des listes de questions qu'on peut poser à un LLM pour vérifier ses capacités.\\n  Donc là, par exemple, petite question de philosophie.\\n  Il faut remplacer avec le bon terme.\\n  Le problème, c'est que c'est déjà arrivé que\\n  des modèles cartonnent en théorie avec des scores de fous, mais en fait,\\n  ne soient pas dingues.\\n  Ça arrive assez régulièrement.\\n  Par exemple, c'est potentiellement le cas des modèles de Google.\\n  Genre Gemini.\\n  On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n\",\n",
       "  \" Pour commencer, ce que je vous propose, c'est de regarder un tableau des meilleures intelligences artificielles qui sont concurrentes à ChatGPT.\\n  Vous allez voir, il y a plein de trucs très intéressants dans ce tableau.\\n  Par exemple, on dirait que ChatGPT régresse entre plusieurs versions.\\n  Sinon, on peut voir qu'il y a aussi des scores qui sont incohérents, qui ne sont pas dans le bon ordre, c'est bizarre.\\n  Et surtout, il y a ces petites lignes jaunes.\\n  Open Hermès Mistral, Machin.\\n  Mixtral Instruct.\\n  Que des noms qui évoquent le vent finalement.\\n  Elles ne payent pas de mine.\\n  On dirait même comme ça qu'elles ne sont pas si bien classées.\\n  Mais...\\n  Ce serait passé à côté de la révolution.\\n  Et je pèse mes mots qui se cachent derrière.\\n  Déjà, il faut réaliser qu'il y a pas mal de manières de mesurer la performance d'un LLM.\\n  Mais pour faire court, c'est pas simple.\\n  Vraiment pas simple.\\n  Il y a des benchmarks qui sont en gros des listes de questions qu'on peut poser à un LLM pour vérifier ses capacités.\\n  Donc là, par exemple, petite question de philosophie.\\n  Il faut remplacer avec le bon terme.\\n  Le problème, c'est que c'est déjà arrivé que\\n  des modèles cartonnent en théorie avec des scores de fous, mais en fait,\\n  ne soient pas dingues.\\n  Ça arrive assez régulièrement.\\n  Par exemple, c'est potentiellement le cas des modèles de Google.\\n  Genre Gemini.\\n  On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n\",\n",
       "  \" Open Hermès Mistral, Machin.\\n  Mixtral Instruct.\\n  Que des noms qui évoquent le vent finalement.\\n  Elles ne payent pas de mine.\\n  On dirait même comme ça qu'elles ne sont pas si bien classées.\\n  Mais...\\n  Ce serait passé à côté de la révolution.\\n  Et je pèse mes mots qui se cachent derrière.\\n  Déjà, il faut réaliser qu'il y a pas mal de manières de mesurer la performance d'un LLM.\\n  Mais pour faire court, c'est pas simple.\\n  Vraiment pas simple.\\n  Il y a des benchmarks qui sont en gros des listes de questions qu'on peut poser à un LLM pour vérifier ses capacités.\\n  Donc là, par exemple, petite question de philosophie.\\n  Il faut remplacer avec le bon terme.\\n  Le problème, c'est que c'est déjà arrivé que\\n  des modèles cartonnent en théorie avec des scores de fous, mais en fait,\\n  ne soient pas dingues.\\n  Ça arrive assez régulièrement.\\n  Par exemple, c'est potentiellement le cas des modèles de Google.\\n  Genre Gemini.\\n  On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n\",\n",
       "  \" Mais...\\n  Ce serait passé à côté de la révolution.\\n  Et je pèse mes mots qui se cachent derrière.\\n  Déjà, il faut réaliser qu'il y a pas mal de manières de mesurer la performance d'un LLM.\\n  Mais pour faire court, c'est pas simple.\\n  Vraiment pas simple.\\n  Il y a des benchmarks qui sont en gros des listes de questions qu'on peut poser à un LLM pour vérifier ses capacités.\\n  Donc là, par exemple, petite question de philosophie.\\n  Il faut remplacer avec le bon terme.\\n  Le problème, c'est que c'est déjà arrivé que\\n  des modèles cartonnent en théorie avec des scores de fous, mais en fait,\\n  ne soient pas dingues.\\n  Ça arrive assez régulièrement.\\n  Par exemple, c'est potentiellement le cas des modèles de Google.\\n  Genre Gemini.\\n  On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n\",\n",
       "  \" Vraiment pas simple.\\n  Il y a des benchmarks qui sont en gros des listes de questions qu'on peut poser à un LLM pour vérifier ses capacités.\\n  Donc là, par exemple, petite question de philosophie.\\n  Il faut remplacer avec le bon terme.\\n  Le problème, c'est que c'est déjà arrivé que\\n  des modèles cartonnent en théorie avec des scores de fous, mais en fait,\\n  ne soient pas dingues.\\n  Ça arrive assez régulièrement.\\n  Par exemple, c'est potentiellement le cas des modèles de Google.\\n  Genre Gemini.\\n  On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n\",\n",
       "  \" des modèles cartonnent en théorie avec des scores de fous, mais en fait,\\n  ne soient pas dingues.\\n  Ça arrive assez régulièrement.\\n  Par exemple, c'est potentiellement le cas des modèles de Google.\\n  Genre Gemini.\\n  On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n\",\n",
       "  \" On dirait qu'en fait, ils ont tout fait pour maximiser leur score de MMLU.\\n  Donc qui est un benchmark très prisé et très regardé.\\n  Sauf que apparemment, quand tu l'utilises, c'est dur d'expliquer pourquoi.\\n  Mais tu sens que c'est quand même moins bon.\\n  Que chat GPT 4 et ça peut parfois s'expliquer parce que l'intero a fuité en gros dans le dataset d'entraînement.\\n  C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n\"],\n",
       " [\" C'est comme si les réponses apparaissaient dans les centaines de gigas de texte que le modèle a appris.\\n  Même si les benchmarks peuvent être intéressants, on n'a pas trouvé mieux actuellement que le feeling des humains pour savoir si un modèle est vraiment bon.\\n  Et un des meilleurs benchmark du coup, c'est l'avis des gens.\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Et surtout, est-ce que tel ou tel modèle est vraiment bon ?\\n  Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n\",\n",
       "  \" Il est bien d'avoir une réponse différente de votre modèle et utiliser vraiment en entreprise ou pas.\\n  Du coup, pour faire des classements, comment on fait ?\\n  Et bien en fait, on peut faire un système de vote.\\n  C'est comme aux échecs, on peut faire un ELO, donc un système de points pour comparer des réponses différentes de modèle.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n\",\n",
       "  \" C'est l'un des classements de ce type les plus connus.\\n  Ce qu'on voit, Arena, ELO, en réalité, ça décrit toutes les batailles qui ont été effectuées sur une audience cible entre différentes classes.\\n  Et en fait, on y revient à ce tableau.\\n  C'est l'un des classements de ce type les plus connus.\\n  C'est l'un des classements de ce type les plus connus.\\n  Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n\",\n",
       "  \" Et là, il faut vous dire qu'on voit vraiment le top du top.\\n  C'est-à-dire que cette liste continue en dessous à l'infini.\\n  Même les petites lignes jaunes, on a l'impression qu'elles sont en bas.\\n  Non, non, non, c'est vraiment le podium du podium des tout meilleurs modèles dispo là au moment où on tourne cette émission\\n  qui ont été testées sur le site en question qui est très populaire.\\n  On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n\",\n",
       "  \" On peut voir que pour l'instant, les tout meilleurs modèles en haut, ils sont tous propriétaires.\\n  Donc, on reconnaît les GPT-4 que tout le monde connaît évidemment.\\n  Ensuite, on peut voir Cloud d'Anthropik.\\n  On n'en a pas beaucoup parlé, mais ça a été monté par d'anciens salariés de OpenAI.\\n  Puis, quelques versions de GPT 3.5 qui, après des mises à jour successives, restent en fait toujours très compétitives actuellement.\\n  Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n\",\n",
       "  \" Et plus bas, on aperçoit Google avec le Gemini Pro, leur nouveau modèle annoncé il y a deux semaines à peine.\\n  Tout ça donc, c'est ce qui est propriétaire.\\n  Ok ?\\n  Après, pour ce qui nous intéresse le plus nous, il y a les modèles ouverts.\\n  Ils sont en général plus petits.\\n  On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n\",\n",
       "  \" On va expliquer ce que ça veut dire juste après.\\n  Ils demandent donc moins de puissance de calcul.\\n  On peut les télécharger gratuitement, les faire tourner en local et les réentraîner,\\n  ce qui est un des trucs les plus intéressants, sur nos propres données pour les rendre vraiment très très très très très forts.\\n  Et alors, jusqu'à il y a quelques semaines, il n'y avait en gros qu'une seule alternative.\\n  Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n\",\n",
       "  \" Et c'est ça le point qui est sérieux.\\n  A Tchad GPT et ses variations, dont on avait déjà parlé, c'est l'YAMA 2.\\n  Plus précisément, des versions améliorées, donc fine-tunées de l'YAMA 2, le modèle de Facebook,\\n  qui finissent d'optimiser au max du max le travail qu'a fait faire dans son modèle dit de fondation.\\n  Mais, il y a deux mois exactement, il y a des petites lignes jaunes qui se sont ajoutées au tableau.\\n  Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n\"],\n",
       " [\" Ça s'est passé comment ?\\n  Mistral, le compte Mistral, qui n'était suivi alors par quasiment personne,\\n  a publié un tweet.\\n  Pour ceux qui ne savent pas ce que c'est, c'est un lien magnet.\\n  Donc c'est tout simplement un torrent qu'on peut télécharger avec, bah voilà, un bit torrent quoi,\\n  comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n\",\n",
       "  \" comme on téléchargerait un film piraté ou alors plein d'autres trucs open source.\\n  Il publie ça. Pas d'explication, rien.\\n  Pas de contexte, pas de vidéo promo, pas de billet de bloc, rien.\\n  Juste ce lien.\\n  Et quand on clique dessus, on découvre un modèle à 7 milliards de paramètres.\\n  Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n\",\n",
       "  \" Et là, il faut qu'on explique un truc très important parce que, je vous l'ai dit,\\n  on peut avoir l'impression que ces lignes jaunes, elles sont en bas du classement.\\n  Mais en fait, ça, c'est si tu ne prends pas en compte la taille des modèles.\\n  C'est comme en boxe, il y a différentes catégories.\\n  C'est-à-dire qu'il y a les poids lourds, il y a les moyens et il y a les poids légers.\\n  Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n\",\n",
       "  \" Et en fait, ce n'est pas du tout la même chose de se battre avec des modèles qui font 200 milliards de paramètres\\n  ou avec des modèles qui font 70 milliards ou 7 milliards.\\n  Ce nombre de milliards décrit en fait la taille des poids.\\n  C'est l'énorme fichier qui contient le réseau neuronal qui permet de faire les inférences,\\n  donc de créer les messages, d'écrire sous vos yeux les tokens.\\n  Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n\",\n",
       "  \" Et donc, plus un modèle est gros, plus il demande de la puissance de calcul,\\n  d'avoir des serveurs gigantesques avec des cartes graphiques de Nvidia qui coûtent 25 000 euros pièce\\n  qu'on cumule pour, à la fin, arriver à héberger des modèles qui vont faire du coup 100 gigas par exemple ou 200 gigas.\\n  Souvent, on ne sait pas exactement en plus quelle est la taille des modèles\\n  propriétaires.\\n  À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n\",\n",
       "  \" À une époque, on pensait que les GPT-3 et compagnie faisaient à peu près 130 milliards de paramètres, si je ne dis pas de bêtises.\\n  GPT-4, c'est sûr que c'est énorme.\\n  Autant dire que héberger sur ça vous-même, sur un de vos appareils, c'est mort.\\n  Dites-vous que c'est juste mort.\\n  C'est pour ça que sont apparus des modèles plus petits.\\n  Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n\",\n",
       "  \" Par exemple, l'YAMA, quand ils ont sorti leurs modèles, ils les ont sortis souvent en trois versions, voire quatre.\\n  Il y a le plus gros.\\n  Il fait 70 milliards de paramètres.\\n  Ça, pour vous donner un ordre d'idée, c'est le plus proche de ce qu'on a qui ressemble à une taille de modèle d'OpenAI ou d'Anthropic.\\n  Et ça, pour le faire tourner, il faut en gros minimum deux cadres graphiques des 4080 Ti.\\n  Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n\",\n",
       "  \" Actuellement, c'était un des moyens d'avoir des modèles quasiment équivalents à un GPT-3.5.\\n  Donc, c'était déjà cool.\\n  Ils ont sorti également des modèles de 30 milliards de paramètres.\\n  De 13 milliards de paramètres.\\n  Et de 7 milliards de paramètres.\\n  Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n\"],\n",
       " [\" Et vous pouvez vous dire à quoi ça sert.\\n  Pourquoi ils ne mettent pas plutôt toute leur énergie, tout leur argent à entraîner un unique modèle qui soit plus fort que tous les autres ?\\n  En gros, ça a un intérêt parce que différents modèles, différentes tailles de modèles, sont utiles pour différents trucs.\\n  Tu peux avoir besoin d'un très gros modèle et donc d'une très bonne compréhension, d'une très grande culture générale pour effectuer certaines actions\\n  en faisant des compromis sur le coût par mot, le coût par token.\\n  Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n\",\n",
       "  \" Et le fait d'avoir des très gros modèles.\\n  Des très grosses infrastructures.\\n  Mais parfois, tu peux avoir des besoins plus restreints que tu es prêt à échanger contre des performances.\\n  Donc, par exemple, si tu veux te faire tourner un modèle sur ton Mac mini qui a 16 gigas de RAM,\\n  tu es très content qu'il y ait des modèles 13 milliards ou 7 milliards.\\n  Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n\",\n",
       "  \" Mais pendant très longtemps, avec un modèle de 7 milliards de paramètres, tu ne faisais quasiment rien.\\n  Pour faire des résumés, ça peut marcher un petit peu.\\n  Ou pour essayer de trouver des synonymes à un mot.\\n  Des choses qui jouent avec le langage, mais à un bas niveau.\\n  C'est un élève de CM2, tu peux dire ça.\\n  Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n\",\n",
       "  \" Mistral.\\n  C'est un modèle de 7 milliards de paramètres.\\n  C'est le plus petit qu'on voit être publié.\\n  Il est dans le top 10.\\n  Sauf qu'en fait, il est complètement dingue.\\n  Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n\",\n",
       "  \" Quand ils l'ont sorti, les gens croyaient à moitié.\\n  On pensait qu'il y avait des bugs quand on voyait les benchmarks.\\n  On s'est dit non, mais ce n'est pas possible.\\n  Ce que je vous expliquais, ils l'ont entraîné sur des benchmarks.\\n  Ça n'a pas de sens.\\n  On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n\",\n",
       "  \" On ne devrait pas pouvoir obtenir ce genre de résultat\\n  avec un modèle qui tient dans un fichier.\\n  Ça n'a pas de sens.\\n  Mais en fait, si.\\n  Leur modèle de 7 milliards, surtout quand il a été fonctionné,\\n  c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n\",\n",
       "  \" c'est un peu les noms, les versions, les open Hermès,\\n  tout ça que vous voyez dans le tableau.\\n  Ce sont des versions améliorées par la communauté\\n  qui ont poussé ce modèle à un niveau où\\n  il explose évidemment tous les 13 milliards,\\n  mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n\",\n",
       "  \" mais également les meilleurs modèles en 70 milliards de paramètres.\\n  C'est-à-dire qu'actuellement, la meilleure déclinaison de Mistral,\\n  en 7 milliards de paramètres, c'est Sterling LM7B Alpha.\\n  Elle explose des GPT 3.5 Turbo,\\n  des PPLX 70 milliards,\\n  donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n\"],\n",
       " [\" donc 70B ça veut dire 70 milliards.\\n  C'est la ligne Sterling qu'il faut regarder.\\n  Exactement.\\n  L'IAMA2 70 milliards.\\n  Je ne sais pas si vous vous rendez compte.\\n  La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n\",\n",
       "  \" La prouesse que c'est.\\n  Et c'est ça qui a expliqué qu'il y a deux mois,\\n  il y a eu une sorte de raz-de-marée\\n  où tout le monde s'est mis à jouer avec ce lien magnète,\\n  à le télécharger, à le fin-tuner, à essayer de l'améliorer\\n  et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n\",\n",
       "  \" et à voir où est-ce qu'on pouvait le pousser au maximum du maximum.\\n  Il y a quand même une petite subtilité à capter,\\n  c'est que ça reste un modèle petit.\\n  Et donc, tu peux avoir des réponses ultra qualitatives,\\n  mais on pense que typiquement au niveau de la quantité d'informations,\\n  d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n\",\n",
       "  \" d'Internet qu'il a pu stocker dans sa mémoire,\\n  on pense qu'il va être peut-être un peu plus limité dans certains cas.\\n  Il peut avoir un risque d'halluciner un peu plus souvent,\\n  d'inventer des trucs qui ne sont pas dans son dataset.\\n  C'est peut-être une toute petite nuance qu'on peut donner\\n  au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n\",\n",
       "  \" au fait d'avoir un modèle de 7 milliards.\\n  Mais à part ça, ça veut dire que ce truc-là,\\n  vous pouvez le faire tourner sur certains iPhones\\n  ou de manière plus réaliste sur votre Mac sans aucun problème.\\n  Ça va tourner à la vitesse de l'éclair,\\n  parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n\",\n",
       "  \" parce que c'est vraiment tout petit,\\n  donc c'est bien plus rapide que vous ne pourriez le lire.\\n  Ça veut dire que des développeurs même d'applications\\n  peuvent maintenant l'intégrer en back-end,\\n  en local complètement, sans avoir la moindre connexion Internet,\\n  avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n\",\n",
       "  \" avoir un quasi GPT 3.5.\\n  Vous avez vu que pour l'instant, je n'ai pas parlé d'une petite ligne.\\n  Il en manque une là.\\n  Ça a à peine 10 jours.\\n  Et c'est une autre forme de révolution.\\n  C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n\",\n",
       "  \" C'est-à-dire qu'ils publient simplement un nouveau magnet.\\n  Et alors là, c'est Noël, tu vois, donc tu ouvres le magnet\\n  et tu regardes ce qu'il y a dedans.\\n  Là, il y a un modèle qui s'appelle Mixtral 7B x 8.\\n  Ce qu'ils ont sorti, c'est un modèle dit de MOE.\\n  Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n\"],\n",
       " [\" Donc ça veut dire Mixture of Experts.\\n  Donc un mélange d'experts.\\n  Ce que tu fais, c'est que tu entraînes différents modèles,\\n  mais qui vont se spécialiser dans des domaines différents.\\n  Pour faire simplifier et schématique,\\n  c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n\",\n",
       "  \" c'est un peu comme si tu entraînais un modèle à être super bon en maths,\\n  un autre à être super bon en code,\\n  un autre à être super bon en littérature et en philosophie.\\n  Dans les faits, c'est quand même beaucoup plus compliqué que ça.\\n  Mais ce que ça permet de faire concrètement,\\n  c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n\",\n",
       "  \" c'est d'entraîner un modèle avec différentes branches.\\n  Et en gros, c'est comme un cerbère à huit têtes,\\n  mais où lors de la génération,\\n  donc pour chaque nouveau token généré,\\n  il y a seulement deux de ces têtes qui sont utilisées.\\n  C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n\",\n",
       "  \" C'est probablement, on est quasiment sûrs,\\n  que OpenAI a utilisé cette architecture sur GPT-4.\\n  Et c'est comme ça qu'ils ont réussi à atteindre ce niveau.\\n  Là où c'est intéressant, c'est que, en gros, pour simplifier,\\n  tu bénéficies de la taille d'un modèle qui fait 8 x 7\\n  sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n\",\n",
       "  \" sans en payer le coût au niveau du hardware.\\n  Donc pour le coût d'un 7 plus 7,\\n  donc pour le coût de 14 milliards de paramètres,\\n  tu bénéficies en quelque sorte de 8 x 7.\\n  En fait, il est juste plus gros en taille,\\n  mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n\",\n",
       "  \" mais il ne nécessite pas plus de puissance de calcul, c'est ça ?\\n  Exactement.\\n  En gros, on peut considérer qu'il est à peu près du niveau de GPT 3.5,\\n  peut-être même un peu au-dessus dans les tests.\\n  Mais ce n'est pas ça le plus fou.\\n  Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n\",\n",
       "  \" Le plus fou, c'est que tu peux le faire tourner littéralement en local\\n  sur un Mac M3 Ultra qui a 64 gigas de RAM.\\n  C'est fou.\\n  C'est la première fois de l'histoire que c'est possible.\\n  Et le deuxième truc de fou, c'est le niveau de performance.\\n  Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n\",\n",
       "  \" Parce que pour l'instant, on a juste parlé de l'intelligence,\\n  mais ce n'est pas la seule chose qui compte.\\n  Il y a la vitesse des tokens aussi qui importe.\\n  La rapidité de réponse.\\n  Exactement. À quel point tu vas vite à répondre.\\n  Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n\"],\n",
       " [\" Et eux, non seulement leur modèle est gigasmart,\\n  mais surtout, il peut répondre à énormément d'utilisateurs en même temps.\\n  Donc en gros, ça veut dire que sur ton Mac qui a 64 gigas de RAM,\\n  tu peux débiter du token comme jamais.\\n  Et pour faire le parallèle plus réaliste,\\n  tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n\",\n",
       "  \" tu es une entreprise et tu veux déployer ta propre version de Mistral.\\n  Et j'ai discuté avec pas mal de boîtes qui sont totalement en train de faire ça actuellement.\\n  C'est en train de prendre Mistral, de les fonctionner sur leur version\\n  et de déployer ça sur leur serveur.\\n  Pour ces entreprises-là, ça va coûter beaucoup, beaucoup moins cher\\n  que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n\",\n",
       "  \" que ça ne coûte à OpenAI de faire la même chose.\\n  Pour faire très, très court.\\n  Je pense que vous réalisez du coup que ma hype n'est pas déplacée.\\n  Surtout que pour l'instant, je vous ai parlé de deux petites révolutions.\\n  Il y en a probablement encore à venir en réalité.\\n  Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n\",\n",
       "  \" Et comment on le sait ?\\n  C'est parce qu'ils ont annoncé récemment leur cloud,\\n  donc leur version hébergée de leur modèle qui s'appelle la plateforme.\\n  J'y ai eu accès et c'est en gros une version de l'API d'OpenAI.\\n  Il y a même une rétrocompatibilité.\\n  C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n\",\n",
       "  \" C'est-à-dire que si tu as développé un service pour OpenAI,\\n  c'est les mêmes endpoints, tout marche pareil.\\n  Tu as juste à changer l'UI.\\n  Tu as juste à changer l'URL et tout va bien.\\n  Et du coup, sur cette plateforme, qu'est-ce qu'on a découvert ?\\n  Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n\",\n",
       "  \" Il y a effectivement les deux modèles qu'ils ont déjà publiés.\\n  Mistral Tiny, je crois.\\n  Mistral Small.\\n  Le Mistral Small, pour eux, c'est la petite ligne jaune tout en haut.\\n  Ok.\\n  Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n\",\n",
       "  \" Mais il y a un troisième modèle, Mistral Medium, qui est en alpha.\\n  Celui-là n'a pas été encore publié.\\n  Ce n'est pas exactement d'ailleurs ce qu'ils vont faire.\\n  Mais ce Mistral Medium,\\n  tu peux déjà essayer les inférences dessus.\\n  Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n\",\n",
       "  \" Donc, il y a accès via une API.\\n  Exactement.\\n  Et en gros, ça promet.\\n  Ça promet d'être encore un sacré morceau.\\n  Il est probablement encore plus gros.\\n  Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n\"],\n",
       " [\" Et au niveau du coût, il est bien plus accessible qu'un GPT-4.\\n  On pense en fait qu'il se situe entre les deux.\\n  C'est-à-dire que ce n'est probablement pas encore exactement l'équivalent d'un GPT-4,\\n  mais que ça va te coûter beaucoup, beaucoup moins cher à l'inférence,\\n  ce qui est leur spécialité.\\n  Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n\",\n",
       "  \" Et rapidement après la découverte de ce Mistral Medium,\\n  il y a pas mal de gens qui ont commencé à faire des trades Twitter\\n  où ils font des comparaisons sur des sujets hyper précis\\n  entre GPT-4 et Mistral Medium.\\n  Parce qu'un truc à préciser, c'est que le GPT-4 qu'on voit tout en haut,\\n  c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n\",\n",
       "  \" c'est les versions de l'API.\\n  Il y a pas mal de gens qui commencent à constater que les versions publiques\\n  de OpenAI, de ChatGPT, deviennent de plus en plus débiles.\\n  Ils essayent des trucs qui marchaient il y a encore six mois, un an.\\n  Leur demander de générer des scripts et des trucs comme ça.\\n  Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n\",\n",
       "  \" Ou à une époque où ça marchait bien.\\n  Ils réessaient aujourd'hui, ça marche beaucoup moins bien.\\n  Ce qui se produit souvent, c'est que pour rendre l'IA safe,\\n  pour éviter qu'elle vienne titiller la sensibilité de quiconque,\\n  Politiquement correcte.\\n  Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n\",\n",
       "  \" Exactement.\\n  On a besoin de les contraindre pour qu'elle réponde\\n  « Je suis une IA, je ne peux pas faire de mal » ou des trucs comme ça.\\n  À chaque fois qu'on contraint un modèle à être safe,\\n  on le rend moins performant.\\n  C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n\",\n",
       "  \" C'est une constante.\\n  C'est-à-dire qu'on l'observe absolument partout.\\n  L'un et l'autre sont un trade-off.\\n  C'est toujours une balance.\\n  Du coup, un exemple frappant que j'ai vu,\\n  c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n\",\n",
       "  \" c'est par exemple un exercice de codage en Python.\\n  La demande qui a été formulée à Mixtral d'un côté et à GPT-4,\\n  c'était écrire un script qui peut rentrer un fichier CSV complet\\n  qui fait un milliard de lignes dans une base de données SQL.\\n  Pas besoin de comprendre vraiment l'énoncé.\\n  Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n\",\n",
       "  \" Dites-vous juste que c'est un problème de programmation non trivial.\\n  En gros, c'est un bon moyen de vérifier si vous avez en face de vous\\n  un élève de 3e ou un PhD.\\n  Parce que la bonne réponse, en fait,\\n  c'est que tu ne peux pas simplement faire une boucle\\n  sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n\"],\n",
       " [\" sur l'ensemble des entrées du CSV\\n  et les rentrées dans une base de données.\\n  Il n'y a aucun système.\\n  Tu n'as pas besoin de contexte supplémentaire\\n  pour savoir que c'est juste impossible.\\n  Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n\",\n",
       "  \" Il te faut une manière d'approcher le problème plus intelligente.\\n  Tu fonctionnes avec des batchs.\\n  Tu fais attention à la gestion de ta mémoire vive, des choses comme ça.\\n  Et il fait la démonstration et montre que d'un côté,\\n  dans l'interface de ChatGPT,\\n  dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n\",\n",
       "  \" dans la version 4,\\n  qu'il est complètement à côté de la plaque,\\n  qu'il bullshite des trucs qui ne servent absolument à rien.\\n  Il passe son temps à te dire\\n  « Non, mais ça, implémente-le toi-même. Commentaire. »\\n  Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n\",\n",
       "  \" Bon, ça, c'est quand même un peu trop compliqué, cette boucle.\\n  Donc, ça demanderait beaucoup plus d'investigation.\\n  Tu vois, pas hyper pertinent.\\n  Tu as besoin de lui reposer des questions en mode\\n  « Non, non, mais vraiment, donne-moi le script complet\\n  qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n\",\n",
       "  \" qui répond à l'énoncé. »\\n  Et là, il finit par y arriver.\\n  Preuve qu'il n'est pas con, juste qu'il est devenu paresseux.\\n  La même demande posée à Mistral Medium.\\n  Et il te pond une réponse, mais...\\n  Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" Oh !\\n  C'est du caviar.\\n  Il n'y a pas un token en trop.\\n  Il ne commence pas à te raconter sa vie, etc.\\n  C'est to the point.\\n  Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" Ça te donne du code qui n'est pas exactement forcément complet,\\n  mais où tu as déjà des briques intéressantes,\\n  à savoir un système de batching.\\n  En gros, il a une profondeur, une compréhension dans l'énoncé.\\n  Et à la fin, il te donne des recommandations pour aller plus loin.\\n  Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" Et là, c'est actionnable.\\n  Tu as des trucs très, très précis qui sont évoqués,\\n  des services, des fonctions dans Python que tu pourrais utiliser, etc.\\n  Et alors, c'est un exemple.\\n  C'est-à-dire que tu peux faire des choses\\n  que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\"],\n",
       " [\" que tu ne veux pas faire.\\n  Ça ne vaut rien.\\n  Ce n'est pas une étude approfondie.\\n  Mais moi, j'ai trouvé ça quand même frappant de se dire,\\n  au moment même où on a l'impression que JPT4 est en train de se prendre\\n  les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" les pieds dans le tapis et de devenir pas ouf,\\n  au même moment, tu vois une courbe comme ça sur la performance\\n  et les capacités de Mistral.\\n  Tout ça pour dire merci, Mistral, d'avoir créé cette boîte.\\n  Merci à eux.\\n  Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" Ils sont juste trop forts.\\n  Suivez-les, s'il vous plaît.\\n  Et franchement, je vais dire, c'est le genre de boîte\\n  qui me rend fier d'être là.\\n  Voilà.\\n  C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" C'est tout simplement fou.\\n  Évidemment, ces nouveautés sont très réjouissantes,\\n  mais j'aimerais amener un petit bémol.\\n  Toutes ces IA sont de plus en plus utilisées\\n  pour les connecter à des plugins,\\n  soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" soit pour se balader sur Internet ou se connecter à vos mails,\\n  vos documents, etc.\\n  Le truc, c'est que beaucoup de gens ne réalisent pas\\n  qu'il y a une vulnérabilité, une faille de sécurité\\n  intrinsèque aux modèles de langage.\\n  C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\",\n",
       "  \" C'est assez flippant et très peu abordé.\\n  Et vous pouvez voir une démonstration pour vous faire un avis\\n  dans cette vidéo.\\n  Sous-titrage Société Radio-Canada\"]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the vectorized database\n",
    "df = pd.read_csv('../rag_db.csv')\n",
    "df.embedding = df.embedding.apply(lambda x: eval(x))\n",
    "\n",
    "def get_file_chunk(filename, df):\n",
    "    chunks = df[df.filename == filename.split('.')[0]].text.tolist()\n",
    "    return chunks\n",
    "\n",
    "def group_chunks_by(chunks, nb):\n",
    "    return [chunks[i:i+nb] for i in range(0, len(chunks), nb)]\n",
    "\n",
    "chunks = get_file_chunk('youtube-2.mp3', df)\n",
    "group_chunks_by(chunks, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name flaubert/flaubert_base_uncased. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from flask import Flask, render_template, redirect, url_for, request\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"\"\"Je vais t'envoyer un texte commencant et finissant par des guillemets.\n",
    "- Tu es un secretaire.  \n",
    "- On veut un compte rendu en markdown de ce texte tire de l'enregistrement d'une video. \n",
    "\n",
    "Instruction importantes:\n",
    "- Ecrit entierement ta reponse en markdown.\n",
    "- Tu dois ecrire en markdown.\n",
    "\n",
    "Tes instructions :\n",
    "- Structure l'information.\n",
    "- Donne un titre au texte.\n",
    "- Decoupe le texte en 2 ou 3 grandes parties en les numerotant et titre les.\n",
    "- Decoupe les parties en sous partie en les numerorant.\n",
    "- Fait un point pour chaque information.\n",
    "- Fait des phrases avec un sujet, un verbe et un complement et un determinant pour les noms pour chaque point.\n",
    "- Ecrit en francais.\n",
    "- Si tu veux ecrire le texte en anglais, ecrit en francais.\n",
    "- On veut que l'information soit structure.\n",
    "- Ecrit entierement ta reponse en markdown, c'est important.\n",
    "- Tu dois ecrire en markdown. \n",
    "- Ne repete pas les memes informations.\n",
    "- Ne fait pas de conclusion.  \n",
    "\"\"\"\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('flaubert/flaubert_base_uncased')\n",
    "\n",
    "# Load the vectorized database\n",
    "df = pd.read_csv('../rag_db.csv')\n",
    "df.embedding = df.embedding.apply(lambda x: eval(x))\n",
    "\n",
    "def find_closest_chunk(query, df, model, n=3):\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    similarity = df.embedding.apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    ids = similarity.sort_values(ascending=False).head(n)\n",
    "    return df.loc[ids.index].text.tolist()\n",
    "\n",
    "def get_file_chunk(filename, df):\n",
    "    chunks = df[df.filename == filename.split('.')[0]].text.tolist()\n",
    "    return chunks\n",
    "\n",
    "def group_chunks_by(chunks, nb):\n",
    "    return [chunks[i:i+nb] for i in range(0, len(chunks), nb)]\n",
    "\n",
    "def make_completion_request(prompt):\n",
    "    url = 'http://localhost:8000/v1/completions'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def make_several_completion_requests(prompts):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(make_completion_request, prompts)\n",
    "    results = list(results)\n",
    "    return results\n",
    "\n",
    "def make_summary(filename):\n",
    "    # Retrieve chunks\n",
    "    chunks = group_chunks_by(get_file_chunk(filename, df), 8)\n",
    "    prompts = []\n",
    "    for chunk in chunks:\n",
    "        prompt = PROMPT + \"\\\"\" + ''.join(chunk) + \"\\\"\\n\\n\"\n",
    "        prompts.append(prompt)\n",
    "    results = make_several_completion_requests(prompts)\n",
    "    summaries = [result['choices'][0]['text'] for result in results]\n",
    "\n",
    "    result = make_completion_request(PROMPT + \"\\\"\" + ''.join(summaries) + \"\\\"\\n\\n\")\n",
    "    return result['choices'][0]['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
